{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H60DVrDg1_Gq"
   },
   "source": [
    "#DRL 2019 Assignment 2: Reinforcement Learning\n",
    "\n",
    "## Total: 150 points\n",
    "\n",
    "\n",
    "**Please do not distribute without permission.**\n",
    "\n",
    "\n",
    "*Special thanks to*: Diana Borsa, Hado van Hasselt, Matteo Hessel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9v_SYckYfv5G"
   },
   "source": [
    "## Content\n",
    "\n",
    "In this assignment, we will investigate the properties of 4 distinct reinforcement learning algorithms:\n",
    "\n",
    "* Policy Evaluation\n",
    "* Online Control: SARSA, Q-learning\n",
    "* Experience Replay\n",
    "* REINFORCE Algorithm \n",
    "\n",
    "Some dimensions of the RL problems we will be considering:\n",
    "* Tabular vs Function Approximation\n",
    "* Off-policy/On-policy Control\n",
    "* Online vs Replay\n",
    "* Exploration vs Explotation\n",
    "\n",
    "\n",
    "## Background reading\n",
    "\n",
    "* Sutton and Barto (2018), Chapters 3-9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNuohp44N00i"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "You will use Python to implement several reinforcement learning algorithms. Only the second part will deal with functional approximation for which we will be using neural networks to approximate value functions and policies.\n",
    "\n",
    "You will then run these algorithms on a few problems, to understand their properties and different emerging behaviour. In this tutorial we will focus primary on fundamental algorithms in RL and explore them in a simple gridworld setting. That being said, these are algorithms that have now been shown to scale very well with (non-linear) functional approximations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztQEQvnKh2t6"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ps5OnkPmDbMX"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=1)\n",
    "plt.style.use('seaborn-notebook')\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ALrRR76eAd6u"
   },
   "source": [
    "## Environments: Grid-Worlds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uMC6nODK1HAV"
   },
   "source": [
    "**(Simple) Tabular Grid-World**\n",
    "\n",
    "You can visualize the grid worlds we will train our agents on, by running the cells below.\n",
    "`S` indicates the start state and `G` indicates the goal.  The agent has four possible actions: up, right, down, and left.  Rewards are: `-5` for bumping into a wall, `+10` for reaching the goal, and `0` otherwise.  The episode ends when the agent reaches the goal, and otherwise continues.  The discount, on continuing steps, is $\\gamma = 0.9$.\n",
    "\n",
    "We will use three distinct GridWorlds:\n",
    "* `Grid` tabular grid world with a goal in the top right of the grid\n",
    "* `AltGrid` tabular grid world with a goal in the bottom left of the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "YP97bVN3NuG8"
   },
   "outputs": [],
   "source": [
    "#@title Environment: Gridworld Implementation\n",
    "class Grid(object):\n",
    "\n",
    "  def __init__(self, discount=0.9, penalty_for_walls=-5):\n",
    "    # -1: wall\n",
    "    # 0: empty, episode continues\n",
    "    # other: number indicates reward, episode will terminate\n",
    "    self._layout = np.array([\n",
    "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "      [-1,  0,  0,  0,  0,  0, -1,  0,  0, -1],\n",
    "      [-1,  0,  0,  0, -1,  0,  0,  0, 10, -1],\n",
    "      [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
    "      [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
    "      [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
    "      [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
    "      [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
    "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
    "    ])\n",
    "    self._start_state = (2, 2)\n",
    "    self._goal_state = (8, 2)\n",
    "    self._state = self._start_state\n",
    "    self._number_of_states = np.prod(np.shape(self._layout))\n",
    "    self._discount = discount\n",
    "    self._penalty_for_walls = penalty_for_walls\n",
    "    self._layout_dims = self._layout.shape\n",
    "\n",
    "  @property\n",
    "  def number_of_states(self):\n",
    "      return self._number_of_states\n",
    "    \n",
    "  def plot_grid(self):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(self._layout <= -1, interpolation=\"nearest\")     \n",
    "    ax = plt.gca()\n",
    "    ax.grid(0)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"The grid\")\n",
    "    plt.text(\n",
    "        self._start_state[0], self._start_state[1], \n",
    "        r\"$\\mathbf{S}$\", ha='center', va='center')\n",
    "    plt.text(\n",
    "        self._goal_state[0], self._goal_state[1], \n",
    "        r\"$\\mathbf{G}$\", ha='center', va='center')\n",
    "    h, w = self._layout.shape\n",
    "    for y in range(h-1):\n",
    "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
    "    for x in range(w-1):\n",
    "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n",
    "\n",
    "  \n",
    "  def get_obs(self):\n",
    "    y, x = self._state\n",
    "    return y*self._layout.shape[1] + x\n",
    "  \n",
    "  def int_to_state(self, int_obs):\n",
    "    x = int_obs % self._layout.shape[1]\n",
    "    y = int_obs // self._layout.shape[1]\n",
    "    return y, x\n",
    "\n",
    "  def step(self, action):\n",
    "    y, x = self._state\n",
    "\n",
    "    if action == 0:  # up\n",
    "      new_state = (y - 1, x)\n",
    "    elif action == 1:  # right\n",
    "      new_state = (y, x + 1)\n",
    "    elif action == 2:  # down\n",
    "      new_state = (y + 1, x)\n",
    "    elif action == 3:  # left\n",
    "      new_state = (y, x - 1)\n",
    "    else:\n",
    "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
    "\n",
    "    new_y, new_x = new_state\n",
    "    if self._layout[new_y, new_x] == -1:  # wall\n",
    "      reward = self._penalty_for_walls\n",
    "      discount = self._discount\n",
    "      new_state = (y, x)\n",
    "    elif self._layout[new_y, new_x] == 0:  # empty cell\n",
    "      reward = 0.\n",
    "      discount = self._discount\n",
    "    else:  # a goal\n",
    "      reward = self._layout[new_y, new_x]\n",
    "      discount = 0.\n",
    "      new_state = self._start_state\n",
    "    \n",
    "    self._state = new_state\n",
    "    return reward, discount, self.get_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "UXyPvOq-S2OT"
   },
   "outputs": [],
   "source": [
    "#@title Alternative Environment: Gridworld (with different goal state)\n",
    "class AltGrid(Grid):\n",
    "  \n",
    "    def __init__(self, discount=0.9, penalty_for_walls=-5):\n",
    "      # -1: wall\n",
    "      # 0: empty, episode continues\n",
    "      # other: number indicates reward, episode will terminate\n",
    "      self._layout = np.array([\n",
    "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "        [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
    "        [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
    "        [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
    "        [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
    "        [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
    "        [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
    "        [-1,  0, 10,  0,  0,  0,  0,  0,  0, -1],\n",
    "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
    "      ])\n",
    "      self._start_state = (2, 2)\n",
    "      self._goal_state = (2, 7)\n",
    "      self._state = self._start_state\n",
    "      self._number_of_states = np.prod(np.shape(self._layout))\n",
    "      self._discount = discount\n",
    "      self._penalty_for_walls = penalty_for_walls\n",
    "      self._layout_dims = self._layout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZVUhh2qqwep_"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise the two environments\n",
    "\n",
    "# Instantiate the two tabular environments\n",
    "grid = Grid()\n",
    "alt_grid = AltGrid()\n",
    "\n",
    "# Plot tabular environments\n",
    "grid.plot_grid()\n",
    "alt_grid.plot_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "rbEydMqDKxZr"
   },
   "outputs": [],
   "source": [
    "#@title Policies (Uniformly random and e-greedy) \n",
    "#Expected syntax: `policy(q_values)` \n",
    "\n",
    "# uniformly random policy\n",
    "def random_policy(q):\n",
    "  return np.random.randint(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cOu9RZY3AkF1"
   },
   "source": [
    "## Helper functions (for visualization and running experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "6EttQGJ1n5Zn"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions for visualisation\n",
    "\n",
    "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
    "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
    "\n",
    "def plot_values(values, colormap='pink', vmin=-1, vmax=10):\n",
    "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
    "  plt.yticks([])\n",
    "  plt.xticks([])\n",
    "  plt.colorbar(ticks=[vmin, vmax])\n",
    "\n",
    "def plot_state_value(action_values):\n",
    "  q = action_values\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "  vmin = np.min(action_values)\n",
    "  vmax = np.max(action_values)\n",
    "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
    "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
    "  plt.title(\"$v(s)$\")\n",
    "\n",
    "def plot_action_values(action_values):\n",
    "  q = action_values\n",
    "  fig = plt.figure(figsize=(8, 8))\n",
    "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "  vmin = np.min(action_values)\n",
    "  vmax = np.max(action_values)\n",
    "  dif = vmax - vmin\n",
    "  for a in [0, 1, 2, 3]:\n",
    "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
    "    \n",
    "    plot_values(q[..., a], vmin=vmin - 0.05*dif, vmax=vmax + 0.05*dif)\n",
    "    action_name = map_from_action_to_name(a)\n",
    "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
    "    \n",
    "  plt.subplot(3, 3, 5)\n",
    "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
    "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
    "  plt.title(\"$v(s)$\")\n",
    "      \n",
    "  \n",
    "def smooth(x, window=10):\n",
    "  return x[:window*(len(x)//window)].reshape(len(x)//window, window).mean(axis=1)\n",
    "  \n",
    "\n",
    "def plot_stats(stats, window=10):\n",
    "  plt.figure(figsize=(16,4))\n",
    "  plt.subplot(121)\n",
    "  xline = range(0, len(stats.episode_lengths), window)\n",
    "  plt.plot(xline, smooth(stats.episode_lengths, window=window))\n",
    "  plt.ylabel('Episode Length')\n",
    "  plt.xlabel('Episode Count')\n",
    "  plt.subplot(122)\n",
    "  plt.plot(xline, smooth(stats.episode_rewards, window=window))\n",
    "  plt.ylabel('Episode Return')\n",
    "  plt.xlabel('Episode Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U86SLdA25GAY"
   },
   "outputs": [],
   "source": [
    "#@title [IMPORTANT] Running the experiments\n",
    "\n",
    "# Simple interaction loop with the MDP:\n",
    "# 1) Interact with the environment\n",
    "# 2) Agent gets observation, rewards, and discount from env. \n",
    "# and is expected to produce the next action\n",
    "def run_experiment(env, agent, number_of_steps):\n",
    "    mean_reward = 0.\n",
    "    try:\n",
    "      action = agent.initial_action()\n",
    "    except AttributeError:\n",
    "      action = 0\n",
    "      \n",
    "    # Interaction wih the MDP\n",
    "    for i in range(number_of_steps):\n",
    "      reward, discount, next_state = env.step(action)\n",
    "      action = agent.step(reward, discount, next_state)\n",
    "      mean_reward += (reward - mean_reward)/(i + 1.)\n",
    "\n",
    "    return mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "aWmvMHR5gM6N"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions for visualizing policies\n",
    "def plot_policy(grid, policy):\n",
    "  action_names = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "  grid.plot_grid()\n",
    "  plt.hold('on')\n",
    "  plt.title('Policy Visualization')\n",
    "  for i in range(9):\n",
    "    for j in range(10):\n",
    "      action_name = action_names[policy[i,j]]\n",
    "      plt.text(j, i, action_name, ha='center', va='center')\n",
    "\n",
    "def plot_greedy_policy(grid, q):\n",
    "  action_names = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "  greedy_actions = np.argmax(q, axis=2)\n",
    "  grid.plot_grid()\n",
    "  plt.hold('on')\n",
    "  plt.title('Greedy Policy')\n",
    "  for i in range(9):\n",
    "    for j in range(10):\n",
    "      action_name = action_names[greedy_actions[i,j]]\n",
    "      plt.text(j, i, action_name, ha='center', va='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fzpb_dGVjT0O"
   },
   "source": [
    "# RL Lab 1: Tabular Agents\n",
    "\n",
    "Each agent, should implement a step function:\n",
    "\n",
    "### `__init__(self, number_of_actions, number_of_states, initial_observation)`:\n",
    "The constructor will provide the agent the number of actions, number of states, and the initial observation. You can get the initial observation by first instatiating an environment, using `grid = Grid()`, and then calling `grid.get_obs()`.\n",
    "\n",
    "Note: All agents should be in pure Python, not TensorFlow needed for this part.\n",
    "\n",
    "### `step(self, reward, discount, next_observation, ...)`:\n",
    "where `...` indicates there could be other inputs (discussed below).  The step should update the internal values, and return a new action to take.\n",
    "\n",
    "When the discount is zero ($\\text{discount} = \\gamma = 0$), then the `next_observation` will be the initial observation of the next episode.  One shouldn't bootstrap on the value of this state, which can simply be guaranteed when using \"$\\gamma \\cdot v(\\text{next_observation})$\" (for whatever definition of $v$ is appropriate) in the update, because $\\gamma = 0$.  So, the end of an episode can be seamlessly handled with the same step function.\n",
    "\n",
    "### `q_values()`:\n",
    "\n",
    "Tabular agents implement a function `q_values()` returning a matrix of Q values of shape: (`number_of_states`, `number_of_actions`)\n",
    "\n",
    "\n",
    "### A note on the initial action\n",
    "Normally, you would also have to implement a method that gives the initial action, based on the initial state.  As in the previous assignment you can use the action `0` (which corresponds to `up`) as initial action, so that otherwise we do not have to worry about this.  Note that this initial action is only executed once, and the beginning of the first episode---not at the beginning of each episode.\n",
    "\n",
    "Q-learning and it's variants needs to remember the last action in order to update its value when they see the next state.  In the `__init__`, make sure you set the initial action to zero, e.g.,\n",
    "```\n",
    "def __init__(...):\n",
    "  (...)\n",
    "  self._action = 0\n",
    "  (...)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B8oKd0oyvNcH"
   },
   "source": [
    "\n",
    "## 1.0: Overview\n",
    "\n",
    "We are going to implement:\n",
    "- Prediction problem: Policy Evaluation\n",
    "- Towards control: Greedy Improvement \n",
    "- Online Tabular SARSA Agent\n",
    "- Online Tabular Q-learning Agent\n",
    "- Tabular Experience Replay Q-learning Agent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5XEP4mf4Jx70"
   },
   "source": [
    "\n",
    "## 1.1: Policy evaluation and Greedy Improvement\n",
    "\n",
    "The purpose here is to evaluate a given policy $\\pi$ -- compute the value function assoicated with following/employing this policy in a given MDP.\n",
    "\n",
    "$$ Q^{\\pi}(S,A) = \\mathbb{E}_{\\tau \\sim P^{\\pi}} [\\sum_t \\gamma^t R_t| s_0=s,a=a_0]$$\n",
    "\n",
    "where $\\tau = \\{s_0, a_0, r_0, s_1, a_1, r_1, \\cdots \\}$\n",
    "\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "**Initialize** $Q(s, a)$ for all s ∈ $\\mathcal{S}$ and a ∈ $\\mathcal{A}(s)$\n",
    "\n",
    "**Loop forever**:\n",
    "\n",
    "1. $S \\gets{}$current (nonterminal) state\n",
    " \n",
    "2. $A \\gets{} \\text{behaviour_policy}(S)$\n",
    " \n",
    "3. Take action $A$; observe resulting reward $R$, discount $\\gamma$, and state, $S'$\n",
    "\n",
    "4. $Q(S, A) \\gets Q(S, A) + \\alpha (R + \\gamma Q(S', \\pi(S')) − Q(S, A))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nORJvcHML9os"
   },
   "outputs": [],
   "source": [
    "# uniformly random policy\n",
    "def random_policy(q):\n",
    "  return np.random.randint(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-KawyBV2_Sxz"
   },
   "source": [
    "### Task 1.1.1 [Coding, 10 points]\n",
    "\n",
    "Complete the code for Policy Evaluation Agent\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_IWIHIvxyC-H"
   },
   "outputs": [],
   "source": [
    "#@title [Coding Task] Policy Evaluation AGENT\n",
    "class PolicyEval_AGENT(object):\n",
    "\n",
    "  def __init__(\n",
    "      self, number_of_states, number_of_actions, initial_state, evaluated_policy, \n",
    "      behaviour_policy=random_policy, step_size=0.1):\n",
    "    self._action = 0\n",
    "    self._state = initial_state\n",
    "    self._number_of_states = number_of_states\n",
    "    self._number_of_actions = number_of_actions\n",
    "    self._step_size = step_size\n",
    "    self._behaviour_policy = behaviour_policy\n",
    "    self._evaluated_policy = evaluated_policy\n",
    "    \n",
    "    self._kill = 0\n",
    "    \n",
    "    # ============ YOUR CODE HERE =============\n",
    "    # initialize your q-values (this is a table of state and action pairs\n",
    "    # Note: this can be random, but the code was tested w/ zero-initialization \n",
    "    # self._q =\n",
    "    self._q = np.zeros((number_of_states, number_of_actions))\n",
    "    \n",
    "  @property\n",
    "  def q_values(self):    \n",
    "    return self._q\n",
    "\n",
    "  def step(self, reward, discount, next_state):\n",
    "    s = self._state\n",
    "    a = self._action\n",
    "    r = reward\n",
    "    g = discount\n",
    "    next_s = next_state\n",
    "    \n",
    "    ##\n",
    "    #self._kill += 1 \n",
    "    #if self._kill > 5:\n",
    "        #return -1\n",
    "    ##\n",
    "    # ============ YOUR CODE HERE =============\n",
    "    # Q-value table update\n",
    "    self._q[s, a] += self._step_size*(r + g*max(self._q[next_s]) - self._q[s, a])\n",
    "    self._state = next_state\n",
    "    \n",
    "    # Get the action to send to execute in the environment and return it\n",
    "    self._action = np.argmax(self._q[s])\n",
    "    \n",
    "    return self._action\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCPr9KzBtFJ3"
   },
   "source": [
    "**Try it!** Run the policy evaluation agent, evaluating the uniformly randon policy  on the AltGrid() environment for $\\texttt{num_steps} = 1e3, 1e5$. \n",
    "\n",
    "Visualise the resulting value functions $Q(s,a)$. Plotting function is provided for you and it takes in a table of q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "mCWq1yKWp76Y",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER 100000 STEPS ...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = int(1e5) # @param\n",
    "\n",
    "# environment\n",
    "grid = AltGrid()\n",
    "\n",
    "# agent \n",
    "agent = PolicyEval_AGENT(\n",
    "    number_of_states=grid._layout.size, \n",
    "    number_of_actions=4, \n",
    "    initial_state=grid.get_obs(),\n",
    "    evaluated_policy=random_policy,\n",
    "    behaviour_policy=random_policy,\n",
    "    step_size=0.1)\n",
    "\n",
    "# run experiment and get the value functions from agent\n",
    "run_experiment(grid, agent, num_steps)\n",
    "\n",
    "# get the q-values\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "\n",
    "# visualize value functions\n",
    "print('AFTER {} STEPS ...'.format(num_steps))\n",
    "plot_action_values(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER 1000 STEPS ...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = int(1e3) # @param\n",
    "\n",
    "# environment\n",
    "grid = AltGrid()\n",
    "\n",
    "# agent \n",
    "agent = PolicyEval_AGENT(\n",
    "    number_of_states=grid._layout.size, \n",
    "    number_of_actions=4, \n",
    "    initial_state=grid.get_obs(),\n",
    "    evaluated_policy=random_policy,\n",
    "    behaviour_policy=random_policy,\n",
    "    step_size=0.1)\n",
    "\n",
    "# run experiment and get the value functions from agent\n",
    "run_experiment(grid, agent, num_steps)\n",
    "\n",
    "# get the q-values\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "\n",
    "# visualize value functions\n",
    "print('AFTER {} STEPS ...'.format(num_steps))\n",
    "plot_action_values(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WN-KBoc2w9Na"
   },
   "source": [
    "### Task 1.1.2 [Coding, 4 points]\n",
    "\n",
    "Compute and Visualise the greedy policy based on the above evaluation, at the end of the training process for $\\texttt{num_steps} = 1e5$.\n",
    "\n",
    "\n",
    "$$ \\pi_{greedy} (a|s) = \\arg\\max_a q^{\\mu}(s,a)$$\n",
    "\n",
    "To do this, complete the code for epsilon_greedy policy below. Remember to break ties randomly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_RbYQ2xmMBiI"
   },
   "outputs": [],
   "source": [
    "# @title [Coding Task] epilson-greedy policy\n",
    "# Input(s): Q(s,:), epsilon\n",
    "# Output:   Sampled action based on epsilon-Greedy(Q(s,:))\n",
    "def epsilon_greedy(q_values, epsilon=0.1):\n",
    "    if np.random.uniform(0,1) < epsilon:\n",
    "        action = np.random.choice(q_values)\n",
    "    else:\n",
    "        action = np.argmax(q_values)\n",
    "    return action\n",
    "  # return the epsilon greedy action  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q5XBEXuqufcy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nicol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:5: MatplotlibDeprecationWarning: pyplot.hold is deprecated.\n",
      "    Future behavior will be consistent with the long-time default:\n",
      "    plot commands add elements without first clearing the\n",
      "    Axes and/or Figure.\n",
      "  \"\"\"\n",
      "c:\\users\\nicol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\__init__.py:910: MatplotlibDeprecationWarning: axes.hold is deprecated. Please remove it from your matplotlibrc and/or style files.\n",
      "  mplDeprecation)\n",
      "c:\\users\\nicol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\rcsetup.py:156: MatplotlibDeprecationWarning: axes.hold is deprecated, will be removed in 3.0\n",
      "  mplDeprecation)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the greedy policy (whatever works for you, \n",
    "# but you should be able to see what the agent would do\n",
    "# at each step/state)  \n",
    "\n",
    "pi = np.zeros(grid._layout_dims, dtype=np.int32)\n",
    "for i in range(grid._layout_dims[0]):\n",
    "  for j in range(grid._layout_dims[1]):\n",
    "    pi[i, j] = epsilon_greedy(q[i, j], epsilon=0.)\n",
    "    \n",
    "plot_policy(grid, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I71bC1vGAArO"
   },
   "source": [
    "###  Task 1.1.3 [Question, 4 points]\n",
    "\n",
    "**Q: ** What do you observe? (Remember that we are evaluating the uniformly random policy)\n",
    "\n",
    "*Answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_hLuA4e_zaiW"
   },
   "source": [
    "### Task 1.1.4 [Question, 4 points]\n",
    "\n",
    "Re-run the same experiment: policy evaluation agent on the Grid() environment for $\\texttt{num_steps} = 1e5$\n",
    "and visualise the resulting value functions and the greedy policy on top of these values at the end of training.\n",
    "\n",
    "**Q: ** What do you observe? \n",
    "- a) How does this policy compare with the optimal one?\n",
    "- b) Try running the training process longer -- what do you observe?\n",
    "\n",
    "*Answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GZS4CfLtzaiX"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = int(1e5) # @param\n",
    "\n",
    "grid = Grid()\n",
    "\n",
    "agent = PolicyEval_AGENT(\n",
    "    number_of_states=grid._layout.size, \n",
    "    number_of_actions=4, \n",
    "    initial_state=grid.get_obs(),\n",
    "    evaluated_policy=random_policy,\n",
    "    behaviour_policy=random_policy,\n",
    "    step_size=0.1)\n",
    "\n",
    "# run experiment and get the value functions from agent\n",
    "run_experiment(grid, agent, num_steps)\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "\n",
    "# visualise value functions\n",
    "plot_action_values(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i1cC9EZJzaia"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nicol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:16: MatplotlibDeprecationWarning: pyplot.hold is deprecated.\n",
      "    Future behavior will be consistent with the long-time default:\n",
      "    plot commands add elements without first clearing the\n",
      "    Axes and/or Figure.\n",
      "  app.launch_new_instance()\n",
      "c:\\users\\nicol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\__init__.py:910: MatplotlibDeprecationWarning: axes.hold is deprecated. Please remove it from your matplotlibrc and/or style files.\n",
      "  mplDeprecation)\n",
      "c:\\users\\nicol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\rcsetup.py:156: MatplotlibDeprecationWarning: axes.hold is deprecated, will be removed in 3.0\n",
      "  mplDeprecation)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALUAAACzCAYAAADPCl20AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAADNpJREFUeJztnX9slPUdx193bSkpGLfRpVJSder6wdHMBqSCRcqmC9GYkAwR4iIucxlhRDcU4hQSgTHjZrKECZMhDI3BkY04CWQsGzEwY4R6hF86+IoTBuQYwyJw2lYpPPvjeeoO6LW9e57rfe97n1fyhPb4ft/3uc+973vfe+7zeRrzPA9FcYl4oQNQlKhRUyvOoaZWnENNrTiHmlpxDjW14hzlhQ4g34jID4BZwFVAJfAhsNAYszOP9zkPaDDGfL+f4ycBWwADeEAM6AIWG2M29TH3CHBf8OvPjDH3ZR5dGjhtahF5BpgI3G+M+Xdw27eBzSIyxhhztKABXsq/jDGN3b+IyC3AWyLyNWPMqb4mG2MS/N/cJY2zphaRGuCnwI3GmBPdtxtj3hCRx4AhwbgjwE7gm8BTQCuwHLgWqADWG2OeCcbeDvwymHsBfyXdLCIVwG+A7wD/BU4CZ0XkWuBdoM4Yc1ZEYvir8TRjzN7e4jfG7BWRduA6ETkD/Bq4M7jfncBcY0wq7fFOApYbYxpEZCjwPNCMv+K/DvwCOA7cZox5P5izFXjeGLMxm9zajst76vHAgXRDd2OMecUYcyDtpneNMTcbY/4MvAL83hgzBmgC7hKR+0Xky8Ba4EFjzGhgCvBCYNwfA/XAN/CNfW1wP0eBN4DvBffzLaCtL0MDiMh3gYvAP4GFQC1wS3DEged6mb4EGAzcDDTim3ss8DLww0D/xiDmzX3FUmw4u1Lj70u/qAEQkauAN4NfhwJ/NMY8Ffz+ZjBmCNACfEVEfp42thH4BBgOvC4i3bIe/gp/F/CqMeZz4HMRWRfcDrAC+BXwW/y9/QsZ4r1RRPYEP1cAx4Apxph2EbkbWGCMOR/E+Tz+6puJu4DHjDEX8Ff2lmBeEviHiCwAfgSsDsY4hcum3gmMFJFhxpi24K26EUBEFgHVaWM/Cf4tw38x3G6MaQ/GVgOd+KvsAWPMbd2TRKQWOIVv1liaXlfaz1uBKhG5E39//1CGeC/ZU19GGWkvUPyVuiLD2O77T39B1wHtxpj3RWQf/rvMA8BtGeYXNc5uP4wxSWAZ8KdgiwCAiFyH/3Z8xQpljDkH7AAeC8Z+CXgL3wQ7gK+LyMTg/xqBQ8AI/DMXM0VksIgMBqanaXr4q/Rq/NW8M4eH81dgtohUiEgcmAP8vZfxW4GHRCQuIpXABoLVGv+d4zmgNciRczhragBjzAJgDfCqiOwWkQ+B14C/AU9mmPYAME5E9uOv9n8wxqwLzkBMBZ4Tkb34e+8HjTFHgN8BCfwPhduBw5dpvgzUBeNyYSnwH2APcAB/lf5JL+MXA58De4HdwF+MMa8F/7cZf0u1MsdYrCempaf5R0RmAA8ZY+62IJbx+O8aDcG7iHO4vKe2AhHZBnwVfwtTUETkZWASMN1VQ4Ou1IqDOL2nVkoTNbXiHGpqxTl6/aC4a9cu3XArVjNmzJjY5bf1efbj1ltvzU80ihKSRCLR4+26/VCcQ02tOIeaWnGOUKaOx+NUVlaGC8ARDRtisEWj0DHkbOp4PM6IESOoq6ujqqqqpDVsiMEWDStiyOlegZqaGjo7O+no6KC6upry8uzLSFzRsCEGWzRsiAHP8zIeiUTCwy82v+KIxWJeZWWlV1NT48VisYzjejtc0bAhBls0BjKGRCLh9eTbnFfq9EKoXIuiXNGwIQZbNGyIQc9+KM6hplacQ02tuEeuHxT10KPQR+QfFBXFVtTUinOoqRXnUFMrzjEgl0gI07Eei8VCaYSdb6NGWFx5HJnQlVpxDjW14hwFr6dOpVK89957oTSiwIY4UqkUBw8eDKVhw3MSRS6Ltp46lUrx8MMPM2PGDLZv355rKKGxJY4jR47w0ksv5TzfhuckilwWdT31ggULaGxsZNy4cSxbtoxkMvcryx48eJBjx47lNNeWOMJiw3MSRS6Lop46Ex0dHd6+ffu8J554wuvs7OxxTF8a3ezZs8e79957vaNHj2Y9v684+htD2Di6Y8hEbznO5jnpjbC5yOY5Dfs4Mn1NnvMpPS+CutnBgwd/8XM2+6eNGzfy4osvXnH7qVOnmDt3Lhs2bCjKOMJSyOckqvkQ/nEU5aV8p0yZwpQpl14ZN5lMMnv2bJ58MtO11O2OY/fu3ZSVlQGwf/9+RIRBgwZFFmspUZSm7onDhw/z9NNPM3r06KKMY+vWrbzzzjt0dHSwcOFC1qxZQ3V1dd8TlStw5jx1c3NzwQ0dJo758+czZswYPvvsM1atWqWGDkGvF13ftWuXF8W19HLd30HpfU1+4cKFL7YhmTTCYksuwpJIJHq8QKQzK7UrZDK00n/U1IpzqKkV5xiQsx9R7KHCatgQQ1QaNsRgw+PIhK7UinNok0AWGlHgSi5syWdP6EqtOIeaWnGOom4S2LJlC5MnT2bUqFGMHTuWmTNncvHixQGPAwqfCxs02tvbefbZZ2lpaWHUqFFMmDCB2bNn51R+WpJNAqdPn2b+/PmUlZUxfPhwPv30U86cOZPTXi9sYXuhc2GDhud5zJo1i7Vr15JKpfA8jzvuuINkMpm1qUu2SeD48eOcP3+es2fPMmHCBJqbmykvL+fkyZMDGgcUPhc2aOzYsYPW1laGDBnC1KlTaW5uxhjDihUraGhoyCqGkm0SSKVSXlNTk1dfX++JiNfU1OStW7eu3/OziaO3x2ZDLrJ9HFHk4nJWrVrl1dfXe4sWLfL27dvnzZs3z0smk15bW5vX3t7eo0bYfFp50fUwBeVDhw5l/fr1TJ8+nWHDhnHmzBkWL16c09tu2ML2QufCJo2KigrAL8GdNGkS48ePZ/Xq1VlphM1n0Z79OH/+PNdffz1Llixh5cqVjBw5EoBDhw4VOLLSpHuL8fbbb+N5HrW1tcyZM6cgsRRtk8AHH3zA448/zj333IPneXz00UcA1NfXFziy0mTcuHE0NTXR2trK0qVLicfjXH311QWJpWhNXV1dzQ033MD69es5ffo0FRUVPPLII0ycOLHQoZUksViMlStXsmzZMjZt2sTHH3/MiRMnmDx5Mi0tLQMbS297Fm0SuFQjClzJhQ351CYBpWRQUyvOofXUA4wrubAlnz2hK7XiHFpPnYWGLdiQC5vzqSu14hxqasU51NSKcxR1k4BNGlHkwpV8FvovCeRcehqPx726ujrvpptu8qqqqnIqtzx37pw3bdo0r6Ghwdu2bVuPY8Jq9DU/G40oclHIfEaZi1znR5nPTKWnOX9NPnz4cLq6uhg0aBBlZWUkk0m6uroyvnB64tFHH+Waa67h8OHDtLW1sXz5cmpray8Z09en7b40+vNpvb8amcgmF1Fo2JCLKJ7TTPQ3F5m+Ji/aJoH+avQ1PxuNKHJRyHxGmYtc50eZT+eaBGzSiCIXruTThr8koGc/FOdQUyvOoaZWnEObBLLQsAUbcmFDPrVJQCkZ1NSKc2iTQBFiQy5szqeu1IpzaJNAFhq2YEMubM6nrtSKc6ipFedQUyvOoU0CEWlok0C0MWiTgDYJDHgucp0fZT61SUCbBLRJwNMmgSs0oshFIfMZZS5ynR9lPrVJII8aUeTClXxqk4Ci5AE1teIcamrFObRJIAsNW7AhFzbkU5sElJJBTa04hzYJFCE25MLmfOpKrTiHNgmUkIYNMaRr5AtdqRXnUFMrzqH11BFp2BCDLRpaT11E9dSFfBxRaNiWi7De0npqS2qINRdaT+1cDXEhNWzLRVhvaT11HjVsiMEWDa2nVpQ8oKZWnENNrTiH1lOXkIYNMaRrhEXrqZWSQU2tOIeaWnEObRIoQQ0bYsgnulIrzqFNAiWkYUMM6Rr5QldqxTnU1IpzaJNARBo2xGCLhjYJlFhhfCE1bMtFWG9pk4AlhfGaC20ScK4wvpAatuUirLe0SSCPGjbEYIuGNgkoSh5QUyvOoaZWnEObBEpIw4YY0jXCok0CSsmgplacQ+upS1DDhhjyia7UinNoPXUJadgQQ7pGvtCVWnEONbXiHGpqxTm0SSAiDRtisEVDmwRKrDC+kBq25SKst7RJwJLCeM2FNgk4VxhfSA3bchHWW9okkEcNG2KwRUObBBQlD6ipFedQUyvOoU0CJaRhQwzpGmHRJgGlZFBTK86hTQIlqGFDDPlEV2rFOdTUinOoqRXnUFMrzlHwempXNGyIwRaNQseQs6nj8TgjRoygrq6OqqqqktawIQZbNKyIIad7BWpqaujs7KSjo4Pq6mrKy7M/O+iKhg0x2KJhQwwDUk/tuoYNMdiiMZAxWFlP7YqGDTHYomFDDHr2Q3EONbXiHGpqxT1y/aCohx6FPiL/oKgotqKmVpxDTa04h5pacY4+v39MJBIDEYeiREav3eSKUozo9kNxDjW14hxqasU51NSKc6ipFef4H4j5B0HacbcHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualise the greedy policy\n",
    "plot_greedy_policy(grid, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dbODeOkjyUEm"
   },
   "source": [
    "## 1.2 On-policy control: SARSA Agent\n",
    "In the following we are going to be concerned with the control problem -- inferring the optimal value/policy that will 'solve' the MDP. The first algorithm we are going to be looking at is SARSA. \n",
    "\n",
    "Note: This is an **on-policy algoritm** -- i.e: the data collection is done on-policy.\n",
    "\n",
    "\n",
    "**Initialize** $Q(s, a)$ for all s ∈ S and a ∈ A(s)\n",
    "\n",
    "**Loop forever**:\n",
    "\n",
    "1. $S \\gets{}$current (nonterminal) state\n",
    " \n",
    "2. $A \\gets{} \\text{current_policy}(S)$\n",
    " \n",
    "3. Take action $A$; observe resultant reward $R$, discount $\\gamma$, and state, $S'$\n",
    "\n",
    "4. $Q(S, A) \\gets Q(S, A) + \\alpha (R + \\gamma Q(S', A') − Q(S, A))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jtOD2BqAB9el"
   },
   "source": [
    "### Task 1.2.1 [Coding, 14 points]\n",
    "\n",
    "Complete the code for the SARSA Agent\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2g1cXTvXjJ7d"
   },
   "outputs": [],
   "source": [
    "#@title [Coding Task] SARSA Agent\n",
    "class SARSA_AGENT(object):\n",
    "\n",
    "  def __init__(\n",
    "      self, number_of_states, number_of_actions, initial_state, \n",
    "      behaviour_policy, num_offline_updates=0, step_size=0.1):\n",
    "    self._q = np.zeros((number_of_states, number_of_actions))\n",
    "    self._state = initial_state\n",
    "    self._number_of_states = number_of_states\n",
    "    self._number_of_actions = number_of_actions\n",
    "    self._step_size = step_size\n",
    "    self._behaviour_policy = behaviour_policy\n",
    "    self._action = 0\n",
    "    \n",
    "  @property\n",
    "  def q_values(self):\n",
    "    return self._q\n",
    "\n",
    "  def step(self, reward, discount, next_state):\n",
    "    s = self._state\n",
    "    a = self._action\n",
    "    r = reward\n",
    "    g = discount\n",
    "    next_s = next_state\n",
    "    \n",
    "    # ============ YOUR CODE HERE =============\n",
    "    # Q-value table update\n",
    "    # td_error =\n",
    "    # self._q[s, a] =\n",
    "    pass\n",
    "  \n",
    "    # Get the action to send to execute in the environment and return it\n",
    "    self._state = next_state\n",
    "    self._action = self._behaviour_policy(self._q[next_state])\n",
    "    \n",
    "    return self._action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KDjmJOD161Hx"
   },
   "source": [
    "###Task 1.2.2 [Question, 6 points]\n",
    "\n",
    "**Consider** the SARSA agent with different levels of exploration. \n",
    "\n",
    "- Moderate exploration: $\\texttt{epsilon} = 0.1$. \n",
    "\n",
    "- Very exploratory strategy: $\\texttt{epsilon} = 0.5, 1.0$.\n",
    "\n",
    "**Q**: Which do you expect, without running the experiment, to do better?  \n",
    "\n",
    "*Answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oUXSSutaDB8v"
   },
   "source": [
    "###Task 1.2.3 [Question, 4 points]\n",
    "\n",
    "**Q** Run multiple times. What do you observe? ($\\texttt{epsilon} = 0.1$)\n",
    " \n",
    " *Answer here* **bold text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qRHL8d6Q5ua5"
   },
   "outputs": [],
   "source": [
    "epsilon = 0.1 #@param\n",
    "num_steps = int(1e5) #@param\n",
    "\n",
    "grid = Grid(discount=0.9, penalty_for_walls=-1.)\n",
    "\n",
    "behavior_policy = lambda qval: epsilon_greedy(qval, epsilon=epsilon)\n",
    "agent = SARSA_AGENT(\n",
    "    number_of_states=grid._layout.size, \n",
    "    number_of_actions=4, \n",
    "    initial_state=grid.get_obs(),\n",
    "    behaviour_policy=behavior_policy,\n",
    "    step_size=0.1)\n",
    "\n",
    "# run experiment and get the value functions from agent\n",
    "run_experiment(grid, agent, num_steps)\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "\n",
    "# visualize value functions\n",
    "print('AFTER {} STEPS ...'.format(num_steps))\n",
    "plot_action_values(q)\n",
    "\n",
    "# visualise the greedy policy\n",
    "plot_greedy_policy(grid, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WxjNg2_O-Dct"
   },
   "source": [
    "## 1.3 Off-policy control: Q-learning Agent\n",
    "\n",
    "Reminder: Q-learning is a very powerful and general algorithm, that enable control (figuring out the optimal policy/value function) both on and off-policy.\n",
    "\n",
    "**Initialize** $Q(s, a)$ for all s ∈ S and a ∈ A(s)\n",
    "\n",
    "**Loop forever**:\n",
    "\n",
    "1. $S \\gets{}$current (nonterminal) state\n",
    " \n",
    "2. $A \\gets{} \\text{behaviour_policy}(S)$\n",
    " \n",
    "3. Take action $A$; observe resultant reward $R$, discount $\\gamma$, and state, $S'$\n",
    "\n",
    "4. $Q(S, A) \\gets Q(S, A) + \\alpha (R + \\gamma \\max_a Q(S', a) − Q(S, A))$\n",
    "\n",
    "\n",
    "\n",
    "### Task 1.3.1 [Coding, 14 points]\n",
    "\n",
    "Complete the code for the Q-learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D9MR0wQ-jS_C"
   },
   "outputs": [],
   "source": [
    "#@title [Coding Task] Q-learning AGENT\n",
    "class QLearning_AGENT(object):\n",
    "\n",
    "  def __init__(\n",
    "      self, number_of_states, number_of_actions, initial_state, \n",
    "      behaviour_policy, num_offline_updates=0, step_size=0.1):\n",
    "    self._q = np.zeros((number_of_states, number_of_actions))\n",
    "    self._state = initial_state\n",
    "    self._action = 0\n",
    "    self._number_of_states = number_of_states\n",
    "    self._number_of_actions = number_of_actions\n",
    "    self._step_size = step_size\n",
    "    self._behaviour_policy = behaviour_policy\n",
    "    pass\n",
    "    \n",
    "  @property\n",
    "  def q_values(self):\n",
    "    pass\n",
    "\n",
    "  def step(self, reward, discount, next_state):\n",
    "    s = self._state\n",
    "    a = self._action\n",
    "    r = reward\n",
    "    g = discount\n",
    "    next_s = next_state\n",
    "    \n",
    "    # ============ YOUR CODE HERE =============\n",
    "    # Q-value table update\n",
    "    pass\n",
    "  \n",
    "    # Get the action to send to execute in the environment and return it\n",
    "    self._state = next_state\n",
    "    self._action = self._behaviour_policy(self._q[next_state])\n",
    "    \n",
    "    return self._action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8cqpXmZta4l-"
   },
   "source": [
    "**Try it!** **Run** your Q-learning agent on the below environment for 1e5 number of steps\n",
    "Keep the rest, to the default values, for this first step. We'll get to experiment with this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kgtCWNNcZJwW"
   },
   "outputs": [],
   "source": [
    "# enviroment\n",
    "grid = Grid(discount=0.9, penalty_for_walls=-1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hduGg4l19acj"
   },
   "outputs": [],
   "source": [
    "epsilon = 1.0 #@param\n",
    "num_steps = int(1e5) #@param\n",
    "\n",
    "# behavior policy\n",
    "behavior_policy = lambda qval: epsilon_greedy(qval, epsilon=epsilon)\n",
    "\n",
    "# agent\n",
    "agent = QLearning_AGENT(\n",
    "    number_of_states=grid._layout.size, \n",
    "    number_of_actions=4, \n",
    "    initial_state=grid.get_obs(),\n",
    "    behaviour_policy=behavior_policy,\n",
    "    step_size=0.1)\n",
    "\n",
    "# run experiment and get the value functions from agent\n",
    "run_experiment(grid, agent, num_steps)\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "\n",
    "# visualise value functions\n",
    "print('AFTER {} STEPS ...'.format(num_steps))\n",
    "plot_action_values(q)\n",
    "\n",
    "# visualise the greedy policy\n",
    "plot_greedy_policy(grid, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iJastp_kcAZC"
   },
   "source": [
    "### Task 1.3.2 [Question, 4 points]\n",
    "\n",
    "Experiment with different levels of 'greediness'\n",
    "\n",
    "**Q** The default was $\\epsilon=1$, what does this correspond to?\n",
    "\n",
    "*Answer here* \n",
    "\n",
    "###Task 1.3.3 [Question, 6 points]\n",
    "\n",
    "**Q** Try also $\\epsilon =0.1, 0.5$. What do you observe? Does the behaviour policy affect the training in any way?\n",
    "\n",
    "*Answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "omzJxb5ds0Iq"
   },
   "source": [
    "## 1.4 Experience Replay\n",
    "\n",
    " Implement an agent that uses **Experience Replay** to learn action values, at each step:\n",
    "* select actions randomly\n",
    "* accumulate all observed transitions *(s, a, r, s')* in the environment in a *replay buffer*,\n",
    "* apply an online Q-learning \n",
    "* apply multiple Q-learning updates based on transitions sampled from the *replay buffer* (in addition to the online updates).\n",
    "\n",
    "**Initialize** $Q(s, a)$ for all s ∈ S and a ∈ A(s)\n",
    "\n",
    "**Loop forever**:\n",
    "\n",
    "1. $S \\gets{}$current (nonterminal) state\n",
    " \n",
    "2. $A \\gets{} \\text{random_action}(S)$\n",
    " \n",
    "3. Take action $A$; observe resultant reward $R$, discount $\\gamma$, and state, $S'$\n",
    "\n",
    "4. $Q(S, A) \\gets Q(S, A) + \\alpha (R + \\gamma \\max_a Q(S, a) − Q(S, A))$\n",
    "\n",
    "5. $\\text{ReplayBuffer}.\\text{append_transition}(S, A, R, \\gamma, S')$\n",
    "\n",
    "6. Loop repeat n times:\n",
    "\n",
    "  1. $S, A, R, \\gamma, S' \\gets \\text{ReplayBuffer}.\\text{sample_transition}()$\n",
    "  \n",
    "  4. $Q(S, A) \\gets Q(S, A) + \\alpha (R + \\gamma \\max_a Q(S', a) − Q(S, A))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8o8k3VEDEuj9"
   },
   "source": [
    "###Task 1.4.1 [Coding, 14 points]\n",
    "\n",
    "Complete the code for the Q-learning Agent with replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TB9e_reb2pJX"
   },
   "outputs": [],
   "source": [
    "#@title [Coding Task] Q-learning AGENT with a simple replay buffer\n",
    "class ReplayQ_AGENT(object):\n",
    "\n",
    "  def __init__(\n",
    "      self, number_of_states, number_of_actions, initial_state, \n",
    "      behaviour_policy, num_offline_updates=0, step_size=0.1):\n",
    "    self._q = np.zeros((number_of_states, number_of_actions))\n",
    "    self._state = initial_state\n",
    "    self._action = 0\n",
    "    self._number_of_states = number_of_states\n",
    "    self._number_of_actions = number_of_actions\n",
    "    self._num_offline_updates = num_offline_updates\n",
    "    self._step_size = step_size\n",
    "    self._behaviour_policy = behaviour_policy\n",
    "    \n",
    "    # initialise replay buffer\n",
    "    self._replay_buffer = []\n",
    "    \n",
    "  @property\n",
    "  def q_values(self):\n",
    "    return self._q\n",
    "\n",
    "  def step(self, reward, discount, next_state):\n",
    "    s = self._state\n",
    "    a = self._action\n",
    "    r = reward\n",
    "    g = discount\n",
    "    next_s = next_state\n",
    "    \n",
    "    # Online Q-value update\n",
    "    td_error = r + g * np.max(self._q[next_s]) - self._q[s, a]\n",
    "    self._q[s, a] += self._step_size * td_error\n",
    "    \n",
    "    # ============ YOUR CODE HERE =============\n",
    "    if self._num_offline_updates > 0:\n",
    "\n",
    "      # Store sample into replay buffer memory\n",
    "      \n",
    "    \n",
    "      # ============ YOUR CODE HERE =============\n",
    "      # Q-value table update based on online sample and offline samples\n",
    "      # This update is the same as the above (Q-learning Agent) but \n",
    "      # now we are going to be using samples from the replay buffer.\n",
    "      # Note: You can COPY this from the above Q_learning Agent\n",
    "      pass\n",
    "\n",
    "    # Get the action to send to execute in the environment and return it\n",
    "    self._state = next_state\n",
    "    self._action = self._behaviour_policy(self._q[next_state])\n",
    "    \n",
    "    return self._action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gPfrr4TbE8FY"
   },
   "source": [
    "**Try it!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F5vnFSWVDU3A"
   },
   "outputs": [],
   "source": [
    "num_offline_updates=30 #@param\n",
    "num_steps = int(1e4) #@param\n",
    "\n",
    "grid = Grid(discount=0.9, penalty_for_walls=-1.)\n",
    "\n",
    "behavior_policy = lambda qval: epsilon_greedy(qval, epsilon=epsilon)\n",
    "agent = ReplayQ_AGENT(\n",
    "    number_of_states=grid._layout.size, \n",
    "    number_of_actions=4, \n",
    "    initial_state=grid.get_obs(),\n",
    "    num_offline_updates=num_offline_updates, \n",
    "    step_size=0.1)\n",
    "\n",
    "# run experiment and get the value functions from agent\n",
    "run_experiment(grid, agent, num_steps)\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "\n",
    "# visualize value functions\n",
    "print('AFTER {} STEPS ...'.format(num_steps))\n",
    "plot_action_values(q)\n",
    "\n",
    "# visualise the greedy policy\n",
    "plot_greedy_policy(grid, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5AWyVs16A-x"
   },
   "source": [
    "## 1.5 Further Analysis:  Data Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWutE_URvT7K"
   },
   "source": [
    "**Online Q-learning**\n",
    "\n",
    "* $\\text{number_of_steps}$ = $1e3$ and $\\text{num_offline_updates}$ = $0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iix-yw-MKS4Y"
   },
   "outputs": [],
   "source": [
    "grid = Grid()\n",
    "agent = ReplayQ_AGENT(\n",
    "  grid._layout.size, 4, grid.get_obs(),\n",
    "  random_policy, num_offline_updates=0, step_size=0.1)\n",
    "run_experiment(grid, agent, int(1e3))\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "plot_action_values(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KZXsXJYBgC_N"
   },
   "source": [
    "**Experience Replay**\n",
    "\n",
    "* $\\text{number_of_steps}$ = $1e3$ and $\\text{num_offline_updates}$ = $30$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ASml5uAeIl4A"
   },
   "outputs": [],
   "source": [
    "grid = Grid()\n",
    "agent = ReplayQ_AGENT(\n",
    "  grid._layout.size, 4, grid.get_obs(),\n",
    "  random_policy, num_offline_updates=30, step_size=0.1)\n",
    "run_experiment(grid, agent, int(1e3))\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "plot_action_values(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aKV7aSsIFU8q"
   },
   "source": [
    "### Task 1.5.1 [Question, 8 points] \n",
    "\n",
    "**Q** Why does ExperienceReplay obtain a better solution, considering it has the same amount of actual interactions with the environment as online Q-learning?\n",
    "\n",
    "*Answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-YdF57kT5Tt"
   },
   "source": [
    "# RL Lab 2: REINFORCE Agent with Function Approximation \n",
    "\n",
    "## REINFORCE Agent\n",
    "\n",
    "We are still trying to solve the control problem: estimate the policy that gives us a better long term (discounted) return:\n",
    "\n",
    "$$G_t = \\sum_{k=t+1} \\gamma^{k-t-1}R_k $$\n",
    "\n",
    "Objective:\n",
    "$$J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}} [G_t]$$\n",
    "\n",
    "\n",
    "Using the log-derivative trick, we can obtain the gradient of $J(\\theta)$ with respect to the policy parameters $\\theta$ as:\n",
    "\n",
    "$$\\nabla_{\\theta} J = \\mathbb{E}_{{\\pi_{\\theta}}}[\\sum_{t}\\nabla_{\\theta}\\log \\pi_{\\theta}(s_t,a_t) G_t]$$\n",
    "\n",
    "We estimate this gradient via sampling episodes in the enviroment.\n",
    "\n",
    "\n",
    "\n",
    "## Functional Approximation involved\n",
    "**Approximate** $ \\pi(s,a)$ for all s ∈ S and a ∈ A(s) via a functional approximator (in our case a sonnet/tensorflow model) and learn this function directly. In the second part we will introduce a baseline in the form a value function $V(s)$ that will be represented by an additional functional approximator.\n",
    "\n",
    "\n",
    "### Reference and Further Reading\n",
    "For futher information and refresher on this algorithm and general policy-gradient approached, please check out Chapter 13: Policy Gradient Methods in Sutton & Barto's  book.Models covered in this tutorial: \n",
    "\n",
    "13.3 REINFORCE: Monte Carlo Policy Gradient (2.1)\n",
    "\n",
    "13.4 REINFORCE with Baseline (2.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8b_qEawj35Y"
   },
   "source": [
    "## 2.1 (Vanilla) REINFORCE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VU1aBWr4u4tc"
   },
   "source": [
    "### 2.1.1 Build agent\n",
    "This is slipt into two part:\n",
    "* Build the Policy functional approximator\n",
    "* Build the REINFORCE Agent\n",
    "\n",
    "Note: In the code below we will be using (by default) very simple transformations (linear layers) as this colab is design to focus on the RL algorithm. Please check out previous colab from the summer school for more reference on specifying more interesting functional approximation instances (convolutional nets, LSTM-s, etc) and feel free to experiment with other, more intricated networks. Whenever designing these, always try to keep in mind what are the properties and requirements for the functions you are trying to approximated -- e.g. whether or not you need longer term dependencies, or whether the current observation is sufficient to estimate the target values/intended transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "nvBpu_fbZtsL"
   },
   "outputs": [],
   "source": [
    "#@title Helper code: Episode Logging\n",
    "# Book keeping of some useful (episode) statistics\n",
    "Logging_EpStats = namedtuple(\"Logging_EpStats\", \n",
    "                             [\"episode_lengths\", \"episode_rewards\"])   \n",
    "\n",
    "# We are going to be storing transitions encountered in the episode to use \n",
    "# later in the update (at the end of the episode, for REINFORCE)\n",
    "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"discount\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VpzmbhlAGG9-"
   },
   "source": [
    "### Task 2.1.1 [Coding, 14 points]\n",
    "\n",
    "Complete the code for the Policy function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "_nFZqgPnne1U"
   },
   "outputs": [],
   "source": [
    "#@title [Coding Task] Policy Approximator\n",
    "#Define a class that build our policy approximation\n",
    "class PolicyApproximator():\n",
    "    \"\"\"\n",
    "    Policy Function approximator. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_actions=4, learning_rate=0.01, scope=\"policy_approximation\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            # This is a function of the state V(state)\n",
    "            self.state = tf.placeholder(tf.int32, [], \"state\")\n",
    "            \n",
    "            # Embbed the state into a one-hot coding\n",
    "            state_one_hot = tf.one_hot(self.state, int(grid._layout.size))\n",
    "            \n",
    "            # We are be evaluating the policy of a (previously) selected action\n",
    "            self.action = tf.placeholder(dtype=tf.int32, name=\"action\")\n",
    "            \n",
    "            # Very simple (linear) transformation of the state to \n",
    "            # \\pi_{\\theta}(a|s) -- this is can be anything you think\n",
    "            # your solution class needs to span the intermediate sol.\n",
    "            self.output_layer = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(state_one_hot, 0),\n",
    "                num_outputs=num_actions,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer)\n",
    "            \n",
    "\n",
    "            # ============ YOUR CODE HERE =============\n",
    "            # Compute pi(a=self.action|s) for a selected action\n",
    "            # self.action_probs =\n",
    "            \n",
    "            # Define the loss\n",
    "            # self.loss = \n",
    "            \n",
    "            # Define an optimizer\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            \n",
    "            # Define the training operation\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "      # ============ YOUR CODE HERE =============\n",
    "      sess = sess or tf.get_default_session()\n",
    "      # compute the probabilities of all actions given \n",
    "      # this state \\pi(a|s) for all a.\n",
    "      #pi_a_s = \n",
    "      return pi_a_s\n",
    "\n",
    "    def update(self, state, target, action, sess=None):\n",
    "      # ============ YOUR CODE HERE =============\n",
    "      sess = sess or tf.get_default_session()\n",
    "      # perform training/operation operation\n",
    "      # compute and return the loss\n",
    "      return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hyJNk3WPGVBr"
   },
   "source": [
    "### Task 2.1.2 [Coding, 12 points]\n",
    "\n",
    "Complete the code for the REINFORCE Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "senpTa9-p3FO"
   },
   "outputs": [],
   "source": [
    "#@title [Coding Task] REINFORCE AGENT\n",
    "class REINFORCE_AGENT(object):\n",
    "  \n",
    "  def __init__(\n",
    "      self, number_of_states, number_of_actions, initial_state, \n",
    "      policy_learning_rate=0.01, const_discount=1.0):\n",
    "    \n",
    "    self._policy_approximator = PolicyApproximator(num_actions=number_of_actions,\n",
    "                                                   learning_rate=policy_learning_rate)\n",
    "    self._constant_discount = const_discount \n",
    "    \n",
    "    # initial state/action\n",
    "    self._state = initial_state\n",
    "    self._action = 0\n",
    "    \n",
    "  def step(self, state):\n",
    "    # ============ YOUR CODE HERE =============\n",
    "    # Get the action probabilities \n",
    "    # action_probs = \n",
    "    \n",
    "    # Select you action\n",
    "    # action = \n",
    "    \n",
    "    # Update the internal variables\n",
    "    self._action = action\n",
    "    self._state = state\n",
    "    return self._action\n",
    "    \n",
    "  def update(self, episode):\n",
    "    # go over the all experience collected in this episode\n",
    "    for t, transition in enumerate(episode):\n",
    "      \n",
    "      # ============ YOUR CODE HERE =============\n",
    "      # Compute the (discounted) return\n",
    "      # discounted_return = \n",
    "      \n",
    "      # Update our policy estimator based on return\n",
    "      # self._policy_approximator.update(...)\n",
    "      \n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KL9i62GGhZno"
   },
   "outputs": [],
   "source": [
    "#@title [IMPORTANT] Run REINFORCE agent with an environment 'env'\n",
    "# Description:\n",
    "# Simple experiment run loop (similar to the one above for tabular experiments)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Expected behaviour \n",
    "# 1) For each episode repeat:\n",
    "#  - Interact with the environment (get observation and discount)\n",
    "#  - Store transition\n",
    "# 2) At the end of the episode, use the stored transition to update agent\n",
    "# Repeat for num_episode\n",
    "# -----------------------------------------------------------------------------\n",
    "# Additional: Log and return episode stastics for plotting later on\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_reinforce(env, agent, num_episodes, \n",
    "                  MAXSTEPS_PER_EPISODE=100, \n",
    "                  REPORT_EVERY_N_STEPS=20):\n",
    "    \"\"\"\n",
    "    Run REINFORCE agent in a MDP especified by 'env'. \n",
    "    (Any agent that follows the same logic and can be plugged in though.)\n",
    "    \n",
    "    Agent requirements:\n",
    "      agent.step(state)\n",
    "      agent.update(episode)\n",
    "    \n",
    "    Enviroment requirements:\n",
    "      env.step(action)\n",
    "    \n",
    "    -----------------------------------------------------------------------\n",
    "    Inputs:\n",
    "        env: gridworld\n",
    "        agent: REINFORCE agent (or alternative)\n",
    "        num_episodes: Number of episodes to run for\n",
    "    \n",
    "    Returns:\n",
    "        Logging_EpStats: episode statistics (episode_length & episode_reward)\n",
    "    \"\"\"\n",
    "\n",
    "    # Book-keeping of some useful (episode) statistics\n",
    "    stats = Logging_EpStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))    \n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "      \n",
    "        # Reset the environment and pick the first action\n",
    "        action = 0 #agent.initial_action()\n",
    "        reward, discount, next_state = env.step(action)\n",
    "        \n",
    "        episode = []\n",
    "        \n",
    "        # One step in the environment\n",
    "        state = next_state\n",
    "        for t in range(MAXSTEPS_PER_EPISODE): \n",
    "            \n",
    "            # Take a step\n",
    "            action = agent.step(state)\n",
    "            reward, discount, next_state = env.step(action)\n",
    "            \n",
    "            # Keep track of the transition\n",
    "            episode.append(Transition(\n",
    "              state=state, action=action, reward=reward, next_state=next_state, discount=discount))\n",
    "            \n",
    "            # Optional: Logging and reporting (live) statistics for this epsiode\n",
    "            \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # (Live) reporting\n",
    "            if ((discount == 0) | (t==MAXSTEPS_PER_EPISODE-1)) & (i_episode%REPORT_EVERY_N_STEPS==0):\n",
    "                # Print out which step we're on, useful for debugging\n",
    "                print(\"Episode {}/{}: Length {} ({})\".format(\n",
    "                i_episode + 1, num_episodes, t, stats.episode_rewards[i_episode - 1]))\n",
    "\n",
    "            if discount == 0: # this signals end of the episode          \n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "    \n",
    "        # Go through the episode and make policy updates\n",
    "        agent.update(episode)\n",
    "      \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dDAgK3pUvLuk"
   },
   "source": [
    "### 2.1.2. Run experiment\n",
    " Ready go! Let's test our agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y2lyi-FZhZnr"
   },
   "outputs": [],
   "source": [
    "# initialise an instance of the environment\n",
    "grid = Grid(discount=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RZ3X6hZohZnu"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "agent = REINFORCE_AGENT(number_of_states=grid._layout.size,\n",
    "                        number_of_actions=4, \n",
    "                        initial_state=grid.get_obs(), \n",
    "                        policy_learning_rate=0.01, \n",
    "                        const_discount=1.0)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    # Note, due to randomness in the policy the number of episodes you need to learn a good\n",
    "    # policy may vary. ~500-1000 should be okay\n",
    "    stats = run_reinforce(grid, agent, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9w_ylGcjVnZ"
   },
   "outputs": [],
   "source": [
    "plot_stats(stats, window=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOxqqWyPf9E5"
   },
   "source": [
    "## 2.2 **Adding** a baseline (via $V(s)$) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SfIkMhcDupVN"
   },
   "source": [
    "### 2.2.1. Build the Agent\n",
    "This is similar to the above agent, but now we have an additional problem of estimating/computing the value function $v(s)$.\n",
    "\n",
    "Your tasks will be:\n",
    "* Implement the value function approximator: prediction/update\n",
    "* Adjust the above agent to include this baseline \n",
    "(Note: make sure you remember to update both your value and policy approx.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PVYymo9mGpjS"
   },
   "source": [
    "### Task 2.2.1 [Coding, 12 points]\n",
    "\n",
    "Complete the code for the value function approximator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "jTC91ChZjf34"
   },
   "outputs": [],
   "source": [
    "#@title [Coding Task] Value function approximator \n",
    "class ValueApproximator():\n",
    "    \"\"\"\n",
    "    Value Function approximator. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, scope=\"value_approximation\"):\n",
    "        with tf.variable_scope(scope):\n",
    "          \n",
    "            # This is a function of the state V(state)\n",
    "            self.state = tf.placeholder(tf.int32, [], \"state\")\n",
    "            # Embbed the state into a one-hot coding\n",
    "            state_one_hot = tf.one_hot(self.state, int(grid._layout.size))\n",
    "            \n",
    "            # ============ YOUR CODE HERE =============\n",
    "            # Target Q-value function\n",
    "            # self.target = ...\n",
    "            \n",
    "            # Very simple (linear) transformation of the state to \n",
    "            # the value function V_{\\theta}(s) \n",
    "            \n",
    "            self.output_layer = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(state_one_hot, 0),\n",
    "                num_outputs=1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer)\n",
    "            \n",
    "            self.value_estimate = tf.squeeze(self.output_layer)\n",
    "            \n",
    "            # ============ YOUR CODE HERE =============\n",
    "            # Define the loss\n",
    "            # self.loss = \n",
    "            \n",
    "            # Define an optimizer\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            \n",
    "            # Define the training operation\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.contrib.framework.get_global_step())        \n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "      sess = sess or tf.get_default_session()\n",
    "      # ============ YOUR CODE HERE =============\n",
    "      # v_s = \n",
    "      return v_s\n",
    "\n",
    "    def update(self, state, target, sess=None):\n",
    "      sess = sess or tf.get_default_session()\n",
    "      # ============ YOUR CODE HERE =============\n",
    "      # loss =\n",
    "      return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T87heiQ6G4N-"
   },
   "source": [
    "### Task 2.2.2 [Coding, 12 points]\n",
    "\n",
    "Complete the code for the REINFORCE agent with baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "_gWaK0vctfoG"
   },
   "outputs": [],
   "source": [
    "#@title [Coding Task] REINFORCE with baseline via estimating V(s)\n",
    "class REINFORCE_AGENT(object):\n",
    "  \n",
    "  def __init__(\n",
    "      self, number_of_states, number_of_actions, initial_state, \n",
    "      policy_learning_rate=0.01, value_learning_rate=0.1, const_discount=1.0,\n",
    "      use_baseline=True):\n",
    "    \n",
    "    self._value_approximator  = ValueApproximator(learning_rate=value_learning_rate)\n",
    "    self._policy_approximator = PolicyApproximator(learning_rate=policy_learning_rate)\n",
    "    self._constant_discount = const_discount \n",
    "    \n",
    "    # initial state/action\n",
    "    self._state = initial_state\n",
    "    self._action = 0\n",
    "    self._use_baseline = use_baseline\n",
    "    \n",
    "  def step(self, state):\n",
    "    # ============ YOUR CODE HERE =============\n",
    "    # Note: you an COPY this from above\n",
    "    # Select the action to send to the environment\n",
    "    self._action = action\n",
    "    self._state = state\n",
    "    return self._action\n",
    "    \n",
    "  def update(self, episode):\n",
    "    \n",
    "    # go over the all experience collected in this episode\n",
    "    for t, transition in enumerate(episode):\n",
    "      # ============ YOUR CODE HERE =============\n",
    "      # Compute the discounted return\n",
    "      # discounted_return = \n",
    "      \n",
    "      if self._use_baseline:\n",
    "        # ============ YOUR CODE HERE =============\n",
    "        # Compute baseline/advantage\n",
    "        # baseline = \n",
    "        \n",
    "        # Compute advantage\n",
    "        # advantage = \n",
    "        \n",
    "        # Update our value estimator\n",
    "        # self._value_approximator.update(...)\n",
    "        \n",
    "        # Update our policy estimator\n",
    "        # self._policy_approximator.update(...)\n",
    "        pass\n",
    "      else:\n",
    "        # Update our policy estimator based on return\n",
    "        # self._policy_approximator.update(...)\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k10bD7O9ujp4"
   },
   "source": [
    "### 2.2.2. Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1JXfjIJYsGzF"
   },
   "outputs": [],
   "source": [
    "grid = Grid(discount=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lsKIzTzY81_9"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "agent = REINFORCE_AGENT(number_of_states=grid._layout.size,\n",
    "                        number_of_actions=4, \n",
    "                        initial_state=grid.get_obs(), \n",
    "                        policy_learning_rate=0.01, \n",
    "                        value_learning_rate=0.1, \n",
    "                        const_discount=1.0,\n",
    "                        use_baseline=True)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    # Note, due to randomness in the policy the number of episodes you need to learn a good\n",
    "    # policy may vary. ~2000-5000 seemed to work well.\n",
    "    stats = run_reinforce(grid, agent, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZFg1r7wlJp1H"
   },
   "outputs": [],
   "source": [
    "plot_stats(stats, window=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9e2nE_3BqTWa"
   },
   "source": [
    "### Task 2.2.3 [Question, 8 points]\n",
    "\n",
    "Consider the way in which function approximation was implemented for this agent. How would you say its performance would compare to that of a Tabular agent (after both have converged)? Why?\n",
    "\n",
    "*Answer here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2iC_ADgLxxXL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9v_SYckYfv5G",
    "rNuohp44N00i",
    "ztQEQvnKh2t6",
    "ALrRR76eAd6u",
    "cOu9RZY3AkF1",
    "B8oKd0oyvNcH",
    "5XEP4mf4Jx70",
    "dbODeOkjyUEm",
    "WxjNg2_O-Dct",
    "8cqpXmZta4l-",
    "iJastp_kcAZC",
    "omzJxb5ds0Iq",
    "Q5AWyVs16A-x",
    "VU1aBWr4u4tc",
    "dDAgK3pUvLuk",
    "SfIkMhcDupVN",
    "k10bD7O9ujp4"
   ],
   "name": "DRL2019_Assignment2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
